{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4659434f-e5a8-4918-871f-694643cc7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/toedtli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aab2e2d-d562-4208-8d23-ee955bf6543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], ['from', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'of', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'floppies', 'are', 'especially', 'requested', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'havent', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo'], ['from', 'thomas', 'willis', 'subject', 'pb', 'questions', 'organization', 'purdue', 'university', 'engineering', 'computer', 'network', 'distribution', 'usa', 'lines', 'well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'way', 'back', 'in', 'sooo', 'im', 'in', 'the', 'market', 'for', 'new', 'machine', 'bit', 'sooner', 'than', 'intended', 'to', 'be', 'im', 'looking', 'into', 'picking', 'up', 'powerbook', 'or', 'maybe', 'and', 'have', 'bunch', 'of', 'questions', 'that', 'hopefully', 'somebody', 'can', 'answer', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook', 'introductions', 'are', 'expected', 'id', 'heard', 'the', 'was', 'supposed', 'to', 'make', 'an', 'appearence', 'this', 'summer', 'but', 'havent', 'heard', 'anymore', 'on', 'it', 'and', 'since', 'dont', 'have', 'access', 'to', 'macleak', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had', 'more', 'info', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the', 'ones', 'the', 'duos', 'just', 'went', 'through', 'recently', 'whats', 'the', 'impression', 'of', 'the', 'display', 'on', 'the', 'could', 'probably', 'swing', 'if', 'got', 'the', 'mb', 'disk', 'rather', 'than', 'the', 'but', 'dont', 'really', 'have', 'feel', 'for', 'how', 'much', 'better', 'the', 'display', 'is', 'yea', 'it', 'looks', 'great', 'in', 'the', 'store', 'but', 'is', 'that', 'all', 'wow', 'or', 'is', 'it', 'really', 'that', 'good', 'could', 'solicit', 'some', 'opinions', 'of', 'people', 'who', 'use', 'the', 'and', 'day', 'to', 'day', 'on', 'if', 'its', 'worth', 'taking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', 'realize', 'this', 'is', 'real', 'subjective', 'question', 'but', 'ive', 'only', 'played', 'around', 'with', 'the', 'machines', 'in', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody', 'who', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', 'how', 'well', 'does', 'hellcats', 'perform', 'thanks', 'bunch', 'in', 'advance', 'for', 'any', 'info', 'if', 'you', 'could', 'email', 'ill', 'post', 'summary', 'news', 'reading', 'time', 'is', 'at', 'premium', 'with', 'finals', 'just', 'around', 'the', 'corner', 'tom', 'willis', 'purdue', 'electrical', 'engineering', 'convictions', 'are', 'more', 'dangerous', 'enemies', 'of', 'truth', 'than', 'lies', 'nietzsche'], ['from', 'joe', 'green', 'subject', 're', 'weitek', 'organization', 'harris', 'computer', 'systems', 'division', 'lines', 'distribution', 'world', 'nntp', 'posting', 'host', 'amber', 'ssd', 'csd', 'harris', 'com', 'newsreader', 'tin', 'version', 'pl', 'robert', 'kyanko', 'wrote', 'writes', 'in', 'article', 'anyone', 'know', 'about', 'the', 'weitek', 'graphics', 'chip', 'as', 'far', 'as', 'the', 'low', 'level', 'stuff', 'goes', 'it', 'looks', 'pretty', 'nice', 'its', 'got', 'this', 'quadrilateral', 'fill', 'command', 'that', 'requires', 'just', 'the', 'four', 'points', 'do', 'you', 'have', 'weiteks', 'address', 'phone', 'number', 'id', 'like', 'to', 'get', 'some', 'information', 'about', 'this', 'chip', 'joe', 'green', 'harris', 'corporation', 'computer', 'systems', 'division', 'the', 'only', 'thing', 'that', 'really', 'scares', 'me', 'is', 'person', 'with', 'no', 'sense', 'of', 'humor', 'jonathan', 'winters']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "data = newsgroups_train.data\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "def sent_to_words(sentences):\n",
    "   for sentence in sentences:\n",
    "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:4]) #it will print the data after prepared for stopwords\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc)) \n",
    "   if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "      doc = nlp(\" \".join(sent))\n",
    "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(\n",
    "   data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    ")\n",
    "#print(data_lemmatized[:4]) #it will print the lemmatized data.\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "#print(corpus[:4]) #it will print the corpus we created above.\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
    "#it will print the words with their frequencies.\n",
    "lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "   corpus=corpus, id2word=id2word, num_topics=20,chunksize=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc9306b-c81d-45d5-89b7-8a223210dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.998*\"ax\" + 0.067*\"max\" + 0.001*\"qax\" + 0.001*\"bhj_bhj\" + 0.000*\"giz_giz\" '\n",
      "  '+ 0.000*\"ei_ei\" + 0.000*\"_\" + 0.000*\"pl_pl\" + 0.000*\"qq\" + 0.000*\"pne\"'),\n",
      " (1,\n",
      "  '0.237*\"file\" + 0.228*\"say\" + 0.185*\"go\" + 0.175*\"get\" + 0.162*\"know\" + '\n",
      "  '0.160*\"people\" + 0.141*\"make\" + 0.136*\"use\" + 0.133*\"see\" + 0.130*\"also\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lsi_model.print_topics())\n",
    "doc_lsi = lsi_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a360218a-4222-4c6a-b392-3207b89081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af477fb1-563d-4379-9c99-b6547579b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f31c33-8b47-4629-a0e4-c40e1e65b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenData",
   "language": "python",
   "name": "opendata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
